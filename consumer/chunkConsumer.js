import { consumer } from "./kafkaConsumer.js";

const TOPIC = "file-chunks";

// Estructura temporal para rearmar archivos
const fileBuffers = new Map();

export async function startChunkConsumer() {
  await consumer.subscribe({
    topic: TOPIC,
    fromBeginning: true,
  });

  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      const data = JSON.parse(message.value.toString());

      const {
        fileId,
        chunkIndex,
        totalChunks,
        data: chunkBase64,
      } = data;

      const buffer = Buffer.from(chunkBase64, "base64");

      if (!fileBuffers.has(fileId)) {
        fileBuffers.set(fileId, {
          chunks: new Array(totalChunks),
          received: 0,
        });
      }

      const fileEntry = fileBuffers.get(fileId);

      fileEntry.chunks[chunkIndex] = buffer;
      fileEntry.received++;

      console.log(
        `ðŸ“¦ Chunk recibido: ${chunkIndex + 1}/${totalChunks} (ParticiÃ³n ${partition})`
      );

      // Cuando llega el archivo completo
      if (fileEntry.received === totalChunks) {
        const finalBuffer = Buffer.concat(fileEntry.chunks);

        console.log(`âœ… Archivo completo reconstruido. TamaÃ±o: ${finalBuffer.length} bytes`);

        // AquÃ­ puedes: guardarlo en disco, S3, DB, etc.
        fileBuffers.delete(fileId);
      }
    },
  });
}
